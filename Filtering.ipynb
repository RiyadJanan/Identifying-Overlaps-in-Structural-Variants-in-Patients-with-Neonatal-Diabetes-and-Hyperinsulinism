{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Controls from Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/5211zt715d7151h7fkq093h40000gn/T/ipykernel_82628/1459858627.py:4: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI Case Counts: {'Initial Count': 576293, 'Removed Count (Shared Variants)': 149328, 'Remaining Count (After Shared Variants)': 426965}\n",
      "NDM Case Counts: {'Initial Count': 526289, 'Removed Count (Shared Variants)': 454, 'Remaining Count (After Shared Variants)': 525835}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_clean(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def remove_shared_variants(case_df, control_df):\n",
    "    initial_count = len(case_df)\n",
    "    # Create a set of tuples containing the control variants (Chr, Coordinate1, Coordinate2, Type)\n",
    "    control_variants = set(control_df[['chr', 'Coordinate1', 'Coordinate2', 'Type']].itertuples(index=False, name=None))\n",
    "    # Filter out rows in case_df that are in control_variants\n",
    "    filtered_df = case_df[~case_df.apply(lambda row: (row['chr'], row['Coordinate1'], row['Coordinate2'], row['Type']) in control_variants, axis=1)]\n",
    "    removed_count = initial_count - len(filtered_df)\n",
    "    remaining_count = len(filtered_df)\n",
    "    return filtered_df, removed_count, remaining_count\n",
    "\n",
    "# Load the data\n",
    "controls = load_and_clean('Controls/Controls_V1.csv')\n",
    "hi_cases = load_and_clean('HI_V1.csv')\n",
    "ndm_cases = load_and_clean('NDM_V1.csv')\n",
    "\n",
    "# Remove shared variants\n",
    "filtered_hi_cases, hi_removed, hi_remaining = remove_shared_variants(hi_cases, controls)\n",
    "filtered_ndm_cases, ndm_removed, ndm_remaining = remove_shared_variants(ndm_cases, controls)\n",
    "\n",
    "# Save the filtered data after removing shared variants\n",
    "filtered_hi_cases.to_csv('HI_V2.csv', index=False)\n",
    "filtered_ndm_cases.to_csv('NDM_V2.csv', index=False)\n",
    "\n",
    "# Output the counts\n",
    "hi_counts = {\n",
    "    \"Initial Count\": len(hi_cases),\n",
    "    \"Removed Count (Shared Variants)\": hi_removed,\n",
    "    \"Remaining Count (After Shared Variants)\": hi_remaining\n",
    "}\n",
    "\n",
    "ndm_counts = {\n",
    "    \"Initial Count\": len(ndm_cases),\n",
    "    \"Removed Count (Shared Variants)\": ndm_removed,\n",
    "    \"Remaining Count (After Shared Variants)\": ndm_remaining\n",
    "}\n",
    "\n",
    "print(\"HI Case Counts:\", hi_counts)\n",
    "print(\"NDM Case Counts:\", ndm_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25kb Variants - V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\University\\AppData\\Local\\Temp\\ipykernel_29664\\1468906388.py:4: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI Case Counts: {'Initial Count': 426965, 'Removed Count (Large Variants)': 178477, 'Final Remaining Count': 248488}\n",
      "NDM Case Counts: {'Initial Count': 525835, 'Removed Count (Large Variants)': 172378, 'Final Remaining Count': 353457}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_clean(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def remove_large_variants(df):\n",
    "    initial_count = len(df)\n",
    "    # Filter variants with Read_Width within the range -25000 to 25000\n",
    "    filtered_df = df[(df['Read_Width'] <= 25000) & (df['Read_Width'] >= -25000)]\n",
    "    removed_count = initial_count - len(filtered_df)\n",
    "    remaining_count = len(filtered_df)\n",
    "    return filtered_df, removed_count, remaining_count\n",
    "\n",
    "# Load the data\n",
    "hi_cases = load_and_clean('HI_V2.csv')\n",
    "ndm_cases = load_and_clean('NDM_V2.csv')\n",
    "\n",
    "# Remove large variants\n",
    "filtered_hi_cases, hi_large_removed, hi_large_remaining = remove_large_variants(hi_cases)\n",
    "filtered_ndm_cases, ndm_large_removed, ndm_large_remaining = remove_large_variants(ndm_cases)\n",
    "\n",
    "# Save the filtered data\n",
    "filtered_hi_cases.to_csv('HI_V3.csv', index=False)\n",
    "filtered_ndm_cases.to_csv('NDM_V3.csv', index=False)\n",
    "\n",
    "# Output the counts\n",
    "hi_counts = {\n",
    "    \"Initial Count\": len(hi_cases),\n",
    "    \"Removed Count (Large Variants)\": hi_large_removed,\n",
    "    \"Final Remaining Count\": hi_large_remaining\n",
    "}\n",
    "\n",
    "ndm_counts = {\n",
    "    \"Initial Count\": len(ndm_cases),\n",
    "    \"Removed Count (Large Variants)\": ndm_large_removed,\n",
    "    \"Final Remaining Count\": ndm_large_remaining\n",
    "}\n",
    "\n",
    "print(\"HI Case Counts:\", hi_counts)\n",
    "print(\"NDM Case Counts:\", ndm_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying DGV Overlaps via Bedtools on the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort -k1,1 -k2,2n dgvGold_unique.bed > dgvGold_sorted.bed\n",
    "\n",
    "bedtools merge -i dgvGold_sorted.bed > dgvGold_merged.bed\n",
    "\n",
    "bedtools intersect -a /mnt/data/NDM_V3_corrected_from_csv.bed -b dgvGold_merged.bed -f 0.9 > NDM_DGV_common_strict.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting negative coordinates and adding chr prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "ndm_v3_path = 'NDM_V3.csv'\n",
    "hi_v3_path = 'HI_V3.csv'\n",
    "\n",
    "ndm_v3_data = pd.read_csv(ndm_v3_path)\n",
    "hi_v3_data = pd.read_csv(hi_v3_path)\n",
    "\n",
    "# Function to correct coordinates and add 'chr' prefix\n",
    "def correct_coordinates(data):\n",
    "    # Swap start and end positions where necessary\n",
    "    data.loc[data['Coordinate2'] < data['Coordinate1'], ['Coordinate1', 'Coordinate2']] = data.loc[data['Coordinate2'] < data['Coordinate1'], ['Coordinate2', 'Coordinate1']].values\n",
    "    # Prefix chromosome numbers with 'chr'\n",
    "    data['Chr'] = 'chr' + data['Chr'].astype(str)\n",
    "    return data\n",
    "\n",
    "# Correct both datasets\n",
    "corrected_ndm_v3_data = correct_coordinates(ndm_v3_data)\n",
    "corrected_hi_v3_data = correct_coordinates(hi_v3_data)\n",
    "\n",
    "# Save the corrected data to new CSV files\n",
    "corrected_ndm_v3_path = 'NDM_V3_corrected.csv'\n",
    "corrected_hi_v3_path = 'HI_V3_corrected.csv'\n",
    "\n",
    "corrected_ndm_v3_data.to_csv(corrected_ndm_v3_path, index=False)\n",
    "corrected_hi_v3_data.to_csv(corrected_hi_v3_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Unique Variants from DGV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique intervals for NDM saved to NDM_V4_unique.csv\n",
      "Unique intervals for HI saved to HI_V4_unique.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths to your CSV files\n",
    "ndm_csv_path = 'NDM_V3.csv'\n",
    "hi_csv_path = 'HI_V3.csv'\n",
    "\n",
    "# Load the CSV files\n",
    "ndm_csv_data = pd.read_csv(ndm_csv_path)\n",
    "hi_csv_data = pd.read_csv(hi_csv_path)\n",
    "\n",
    "# Function to correct coordinates and add 'chr' prefix\n",
    "def correct_coordinates(data):\n",
    "    # Swap start and end positions where necessary\n",
    "    data.loc[data['Coordinate2'] < data['Coordinate1'], ['Coordinate1', 'Coordinate2']] = data.loc[data['Coordinate2'] < data['Coordinate1'], ['Coordinate2', 'Coordinate1']].values\n",
    "    # Prefix chromosome numbers with 'chr'\n",
    "    data['Chr'] = 'chr' + data['Chr'].astype(str)\n",
    "    return data\n",
    "\n",
    "# Correct both datasets\n",
    "corrected_ndm_v3_data = correct_coordinates(ndm_csv_data)\n",
    "corrected_hi_v3_data = correct_coordinates(hi_csv_data)\n",
    "\n",
    "# Function to convert to BED format with extra information\n",
    "def convert_to_bed_with_extra_info(data, output_path):\n",
    "    # Ensure correct order of columns: chr, start, end, and extra columns\n",
    "    bed_data = data[['Chr', 'Coordinate1', 'Coordinate2'] + [col for col in data.columns if col not in ['Chr', 'Coordinate1', 'Coordinate2']]]\n",
    "    bed_data.to_csv(output_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "# Define output paths for BED files\n",
    "ndm_bed_path = 'NDM_V4.bed'\n",
    "hi_bed_path = 'HI_V4.bed'\n",
    "\n",
    "# Convert to BED format\n",
    "convert_to_bed_with_extra_info(corrected_ndm_v3_data, ndm_bed_path)\n",
    "convert_to_bed_with_extra_info(corrected_hi_v3_data, hi_bed_path)\n",
    "\n",
    "# Sort the BED files\n",
    "os.system(f\"sort -k1,1 -k2,2n {ndm_bed_path} > {ndm_bed_path.replace('.bed', '_sorted.bed')}\")\n",
    "os.system(f\"sort -k1,1 -k2,2n {hi_bed_path} > {hi_bed_path.replace('.bed', '_sorted.bed')}\")\n",
    "\n",
    "# Define paths for the sorted BED files and the DGV data\n",
    "ndm_bed_sorted_path = ndm_bed_path.replace('.bed', '_sorted.bed')\n",
    "hi_bed_sorted_path = hi_bed_path.replace('.bed', '_sorted.bed')\n",
    "dgv_bed_path = 'DGV/dgvGold.bed'\n",
    "\n",
    "# Ensure DGV data is sorted\n",
    "os.system(f\"sort -k1,1 -k2,2n {dgv_bed_path} > {dgv_bed_path.replace('.bed', '_sorted.bed')}\")\n",
    "dgv_bed_sorted_path = dgv_bed_path.replace('.bed', '_sorted.bed')\n",
    "\n",
    "# Use Bedtools Subtract to find unique intervals\n",
    "os.system(f\"bedtools subtract -a {ndm_bed_sorted_path} -b {dgv_bed_sorted_path} -f 0.9 > {ndm_bed_sorted_path.replace('_sorted.bed', '_unique.bed')}\")\n",
    "os.system(f\"bedtools subtract -a {hi_bed_sorted_path} -b {dgv_bed_sorted_path} -f 0.9 > {hi_bed_sorted_path.replace('_sorted.bed', '_unique.bed')}\")\n",
    "\n",
    "# Paths for the unique BED files\n",
    "ndm_bed_unique_path = ndm_bed_sorted_path.replace('_sorted.bed', '_unique.bed')\n",
    "hi_bed_unique_path = hi_bed_sorted_path.replace('_sorted.bed', '_unique.bed')\n",
    "\n",
    "# Function to convert BED back to CSV\n",
    "def bed_to_csv(bed_path, original_csv, output_csv):\n",
    "    bed_data = pd.read_csv(bed_path, sep='\\t', header=None)\n",
    "    bed_data.columns = original_csv.columns  # Use the original column names\n",
    "    bed_data.to_csv(output_csv, index=False)\n",
    "\n",
    "# Convert the unique BED files back to CSV\n",
    "bed_to_csv(ndm_bed_unique_path, corrected_ndm_v3_data, ndm_bed_unique_path.replace('.bed', '.csv'))\n",
    "bed_to_csv(hi_bed_unique_path, corrected_hi_v3_data, hi_bed_unique_path.replace('.bed', '.csv'))\n",
    "\n",
    "print(f\"Unique intervals for NDM saved to {ndm_bed_sorted_path.replace('_sorted.bed', '_unique.csv')}\")\n",
    "print(f\"Unique intervals for HI saved to {hi_bed_sorted_path.replace('_sorted.bed', '_unique.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove variants shared by NDM & HI - V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDM unique variants by type saved to NDM_V5_unique.csv\n",
      "HI unique variants by type saved to HI_V5_unique.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the split CSV files\n",
    "ndm_split_csv_path = 'NDM_V4_unique.csv'\n",
    "hi_split_csv_path = 'HI_V4_unique.csv'\n",
    "\n",
    "# Load the split CSV files into DataFrames\n",
    "ndm_csv_data = pd.read_csv(ndm_split_csv_path)\n",
    "hi_csv_data = pd.read_csv(hi_split_csv_path)\n",
    "\n",
    "# Function to find unique variants by type\n",
    "def find_unique_variants_by_type(primary_df, subtract_df, variant_type):\n",
    "    primary_df_type = primary_df[primary_df['Type'] == variant_type]\n",
    "    subtract_df_type = subtract_df[subtract_df['Type'] == variant_type]\n",
    "    \n",
    "    # Merge and find unique variants within this type\n",
    "    merged_df = pd.merge(primary_df_type, subtract_df_type, on=['Chr', 'Coordinate1', 'Coordinate2'], how='left', indicator=True)\n",
    "    \n",
    "    # Filter to keep only unique rows from the primary DataFrame\n",
    "    unique_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    # Drop the extra columns that were suffixed with '_y'\n",
    "    unique_df = unique_df.loc[:, ~unique_df.columns.str.endswith('_y')]\n",
    "    \n",
    "    # Rename the '_x' columns back to their original names\n",
    "    unique_df.columns = unique_df.columns.str.replace('_x', '', regex=False)\n",
    "    \n",
    "    return unique_df\n",
    "\n",
    "# Variant types to consider (e.g., 'deletion', 'duplication')\n",
    "variant_types = ndm_csv_data['Type'].unique()\n",
    "\n",
    "# Initialize empty DataFrames to collect unique variants for each type\n",
    "ndm_unique_variants_all = pd.DataFrame()\n",
    "hi_unique_variants_all = pd.DataFrame()\n",
    "\n",
    "# Loop over each variant type\n",
    "for variant_type in variant_types:\n",
    "    # Find NDM unique variants by subtracting HI variants of the same type\n",
    "    ndm_unique_variants = find_unique_variants_by_type(ndm_csv_data, hi_csv_data, variant_type)\n",
    "    \n",
    "    # Find HI unique variants by subtracting NDM variants of the same type\n",
    "    hi_unique_variants = find_unique_variants_by_type(hi_csv_data, ndm_csv_data, variant_type)\n",
    "    \n",
    "    # Append the results for this variant type\n",
    "    ndm_unique_variants_all = pd.concat([ndm_unique_variants_all, ndm_unique_variants])\n",
    "    hi_unique_variants_all = pd.concat([hi_unique_variants_all, hi_unique_variants])\n",
    "\n",
    "\n",
    "# Save the results back to CSV format\n",
    "ndm_unique_csv_path = 'NDM_V5_unique.csv'\n",
    "hi_unique_csv_path = 'HI_V5_unique.csv'\n",
    "\n",
    "ndm_unique_variants_all.to_csv(ndm_unique_csv_path, index=False)\n",
    "hi_unique_variants_all.to_csv(hi_unique_csv_path, index=False)\n",
    "\n",
    "print(f\"NDM unique variants by type saved to {ndm_unique_csv_path}\")\n",
    "print(f\"HI unique variants by type saved to {hi_unique_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeNovos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDM DeNovo variants saved to NDM_DeNovo.csv\n",
      "HI DeNovo variants saved to HI_DeNovo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the final unique variants CSV files\n",
    "ndm_unique_csv_path = 'NDM_V5_unique.csv'\n",
    "hi_unique_csv_path = 'HI_V5_unique.csv'\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "ndm_csv_data = pd.read_csv(ndm_unique_csv_path)\n",
    "hi_csv_data = pd.read_csv(hi_unique_csv_path)\n",
    "\n",
    "# Function to filter for DeNovo variants (GT == '0/0')\n",
    "def filter_de_novo_variants(df):\n",
    "    de_novo_df = df[df['GT'] == '0/0']\n",
    "    return de_novo_df\n",
    "\n",
    "# Filter for DeNovo variants in both datasets\n",
    "ndm_de_novo_variants = filter_de_novo_variants(ndm_csv_data)\n",
    "hi_de_novo_variants = filter_de_novo_variants(hi_csv_data)\n",
    "\n",
    "# Save the DeNovo variants to new CSV files\n",
    "ndm_de_novo_csv_path = 'NDM_DeNovo.csv'\n",
    "hi_de_novo_csv_path = 'HI_DeNovo.csv'\n",
    "\n",
    "ndm_de_novo_variants.to_csv(ndm_de_novo_csv_path, index=False)\n",
    "hi_de_novo_variants.to_csv(hi_de_novo_csv_path, index=False)\n",
    "\n",
    "print(f\"NDM DeNovo variants saved to {ndm_de_novo_csv_path}\")\n",
    "print(f\"HI DeNovo variants saved to {hi_de_novo_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter by Read Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDM variants filtered by read count saved to NDM_DeNovo5.csv\n",
      "HI variants filtered by read count saved to HI_DeNovo5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the DeNovo variants CSV files\n",
    "ndm_de_novo_csv_path = 'NDM_DeNovo.csv'\n",
    "hi_de_novo_csv_path = 'HI_DeNovo.csv'\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "ndm_csv_data = pd.read_csv(ndm_de_novo_csv_path)\n",
    "hi_csv_data = pd.read_csv(hi_de_novo_csv_path)\n",
    "\n",
    "# Function to filter variants by read count (>= 7)\n",
    "def filter_by_read_count(df):\n",
    "    filtered_df = df[df['Read_Count'] >= 5]\n",
    "    return filtered_df\n",
    "\n",
    "# Apply the filter to both datasets\n",
    "ndm_filtered_variants = filter_by_read_count(ndm_csv_data)\n",
    "hi_filtered_variants = filter_by_read_count(hi_csv_data)\n",
    "\n",
    "# Save the filtered variants to new CSV files\n",
    "ndm_filtered_csv_path = 'NDM_DeNovo5.csv'\n",
    "hi_filtered_csv_path = 'HI_DeNovo5.csv'\n",
    "\n",
    "ndm_filtered_variants.to_csv(ndm_filtered_csv_path, index=False)\n",
    "hi_filtered_variants.to_csv(hi_filtered_csv_path, index=False)\n",
    "\n",
    "print(f\"NDM variants filtered by read count saved to {ndm_filtered_csv_path}\")\n",
    "print(f\"HI variants filtered by read count saved to {hi_filtered_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Variants within disease groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDM shared variants saved to NDM_shared_variants_5.csv\n",
      "HI shared variants saved to HI_shared_variants_5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DeNovo CSV files\n",
    "ndm_csv_data = pd.read_csv('NDM_DeNovo5.csv')\n",
    "hi_csv_data = pd.read_csv('HI_DeNovo5.csv')\n",
    "\n",
    "# Function to identify shared variants with overlaps\n",
    "def identify_shared_variants(df):\n",
    "    # Sort by Chr, Coordinate1, and Coordinate2\n",
    "    df = df.sort_values(by=['Chr', 'Region1', 'Region2'])\n",
    "    \n",
    "    # Create an empty DataFrame to store results\n",
    "    shared_variants = pd.DataFrame()\n",
    "    \n",
    "    # Loop through each chromosome\n",
    "    for chr in df['Chr'].unique():\n",
    "        chr_df = df[df['Chr'] == chr]\n",
    "        \n",
    "        # Initialize a list to hold overlapping groups\n",
    "        overlapping_groups = []\n",
    "        \n",
    "        # Initialize the first group with the first row\n",
    "        current_group = [chr_df.iloc[0]]\n",
    "        \n",
    "        for i in range(1, len(chr_df)):\n",
    "            # Check if the current variant overlaps with the previous one\n",
    "            prev_row = chr_df.iloc[i-1]\n",
    "            curr_row = chr_df.iloc[i]\n",
    "            \n",
    "            if curr_row['Region1'] <= prev_row['Region2']:\n",
    "                # If they overlap, add the current row to the group\n",
    "                current_group.append(curr_row)\n",
    "            else:\n",
    "                # If they don't overlap, finalize the current group and start a new one\n",
    "                overlapping_groups.append(pd.DataFrame(current_group))\n",
    "                current_group = [curr_row]\n",
    "        \n",
    "        # Add the last group\n",
    "        overlapping_groups.append(pd.DataFrame(current_group))\n",
    "        \n",
    "        # For each group of overlapping variants, count unique Sample_IDs\n",
    "        for group in overlapping_groups:\n",
    "            if group['Sample_ID'].nunique() > 1:\n",
    "                shared_variants = pd.concat([shared_variants, group])\n",
    "    \n",
    "    return shared_variants\n",
    "\n",
    "# Apply the function to both datasets\n",
    "ndm_shared_variants = identify_shared_variants(ndm_csv_data)\n",
    "hi_shared_variants = identify_shared_variants(hi_csv_data)\n",
    "\n",
    "# Save the shared variants to new CSV files\n",
    "ndm_shared_csv_path = 'NDM_shared_variants_5.csv'\n",
    "hi_shared_csv_path = 'HI_shared_variants_5.csv'\n",
    "\n",
    "ndm_shared_variants.to_csv(ndm_shared_csv_path, index=False)\n",
    "hi_shared_variants.to_csv(hi_shared_csv_path, index=False)\n",
    "\n",
    "print(f\"NDM shared variants saved to {ndm_shared_csv_path}\")\n",
    "print(f\"HI shared variants saved to {hi_shared_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDM shared variants saved to NDM_shared_variants_5_3.csv\n",
      "HI shared variants saved to HI_shared_variants_5_3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DeNovo CSV files\n",
    "ndm_csv_data = pd.read_csv('NDM_DeNovo5.csv')\n",
    "hi_csv_data = pd.read_csv('HI_DeNovo5.csv')\n",
    "\n",
    "# Function to identify shared variants with overlaps\n",
    "def identify_shared_variants(df, overlap_threshold=0.3):\n",
    "    # Sort by Chr, Region1, and Region2\n",
    "    df = df.sort_values(by=['Chr', 'Region1', 'Region2'])\n",
    "\n",
    "    # Create an empty DataFrame to store results\n",
    "    shared_variants = pd.DataFrame()\n",
    "\n",
    "    # Loop through each chromosome\n",
    "    for chr in df['Chr'].unique():\n",
    "        chr_df = df[df['Chr'] == chr]\n",
    "\n",
    "        # Initialize a list to hold overlapping groups\n",
    "        overlapping_groups = []\n",
    "\n",
    "        # Initialize the first group with the first row\n",
    "        current_group = [chr_df.iloc[0]]\n",
    "\n",
    "        for i in range(1, len(chr_df)):\n",
    "            prev_row = chr_df.iloc[i-1]\n",
    "            curr_row = chr_df.iloc[i]\n",
    "\n",
    "            # Calculate the overlap\n",
    "            overlap_start = max(prev_row['Region1'], curr_row['Region1'])\n",
    "            overlap_end = min(prev_row['Region2'], curr_row['Region2'])\n",
    "            overlap_length = max(0, overlap_end - overlap_start + 1)\n",
    "\n",
    "            # Calculate the lengths of the CNVs\n",
    "            prev_length = prev_row['Region2'] - prev_row['Region1'] + 1\n",
    "            curr_length = curr_row['Region2'] - curr_row['Region1'] + 1\n",
    "\n",
    "            # Calculate the overlap percentage\n",
    "            overlap_percentage = overlap_length / min(prev_length, curr_length)\n",
    "\n",
    "            if overlap_percentage >= overlap_threshold:\n",
    "                # If they overlap enough, add the current row to the group\n",
    "                current_group.append(curr_row)\n",
    "            else:\n",
    "                # If they don't overlap enough, finalize the current group and start a new one\n",
    "                overlapping_groups.append(pd.DataFrame(current_group))\n",
    "                current_group = [curr_row]\n",
    "\n",
    "        # Add the last group\n",
    "        overlapping_groups.append(pd.DataFrame(current_group))\n",
    "\n",
    "        # For each group of overlapping variants, count unique Sample_IDs\n",
    "        for group in overlapping_groups:\n",
    "            if group['Sample_ID'].nunique() > 1:\n",
    "                shared_variants = pd.concat([shared_variants, group])\n",
    "\n",
    "    return shared_variants\n",
    "\n",
    "# Apply the function to both datasets with a 30% overlap threshold\n",
    "ndm_shared_variants = identify_shared_variants(ndm_csv_data, overlap_threshold=0.3)\n",
    "hi_shared_variants = identify_shared_variants(hi_csv_data, overlap_threshold=0.3)\n",
    "\n",
    "# Save the shared variants to new CSV files\n",
    "ndm_shared_csv_path = 'NDM_shared_variants_5_3.csv'\n",
    "hi_shared_csv_path = 'HI_shared_variants_5_3.csv'\n",
    "\n",
    "ndm_shared_variants.to_csv(ndm_shared_csv_path, index=False)\n",
    "hi_shared_variants.to_csv(hi_shared_csv_path, index=False)\n",
    "\n",
    "print(f\"NDM shared variants saved to {ndm_shared_csv_path}\")\n",
    "print(f\"HI shared variants saved to {hi_shared_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out Inversions and Reformat Type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDM filtered data saved to NDM_DUPDEL.csv\n",
      "HI filtered data saved to HI_DUPDEL.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/5211zt715d7151h7fkq093h40000gn/T/ipykernel_56566/991424124.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Type'] = df_filtered['Type'].replace({'Deletion': 'DEL', 'Duplication': 'DUP'})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the CSV files\n",
    "ndm_csv_file_path = 'NDM_shared_variants_5.csv'\n",
    "hi_csv_file_path = 'HI_shared_variants_5.csv'\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "ndm_data = pd.read_csv(ndm_csv_file_path)\n",
    "hi_data = pd.read_csv(hi_csv_file_path)\n",
    "\n",
    "# Function to filter out inversions and convert CNV types\n",
    "def filter_and_convert_cnv(df):\n",
    "    # Filter out rows where 'Type' column contains 'Inversion'\n",
    "    df_filtered = df[~df['Type'].str.contains('Inversion', case=False, na=False)]\n",
    "    \n",
    "    # Replace 'Deletion' with 'DEL' and 'Duplication' with 'DUP' in the 'Type' column\n",
    "    df_filtered['Type'] = df_filtered['Type'].replace({'Deletion': 'DEL', 'Duplication': 'DUP'})\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply the function to both DataFrames\n",
    "filtered_ndm_data = filter_and_convert_cnv(ndm_data)\n",
    "filtered_hi_data = filter_and_convert_cnv(hi_data)\n",
    "\n",
    "# Save the filtered and converted data to new CSV files\n",
    "filtered_ndm_csv_path = 'NDM_DUPDEL.csv'\n",
    "filtered_hi_csv_path = 'HI_DUPDEL.csv'\n",
    "\n",
    "filtered_ndm_data.to_csv(filtered_ndm_csv_path, index=False)\n",
    "filtered_hi_data.to_csv(filtered_hi_csv_path, index=False)\n",
    "\n",
    "print(f\"NDM filtered data saved to {filtered_ndm_csv_path}\")\n",
    "print(f\"HI filtered data saved to {filtered_hi_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = 'HI_DUPDEL.csv'\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Select the required columns for BED format\n",
    "bed_data = data[['Chr', 'Region1', 'Region2', 'Type','Sample_ID','Read_Count','Read_Width']]\n",
    "\n",
    "# Save as a BED file\n",
    "bed_file_path = 'HI_DUPDEL.bed'\n",
    "bed_data.to_csv(bed_file_path, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
